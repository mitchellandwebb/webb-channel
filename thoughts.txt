
So ... we want a channel. This is a channel that may have no buffer, or might have an infinite buffer. If it has a buffer, then once it is full, users who want to send to it must wait to do so, to avoid overflowing the buffer.

We also have receivers. They want to receive; if data exists, then then they can receive immediately, otherwise they wait, holding a receive mutex to block other receivers. The mutex thus serves as the queuing mechanism. Each receiver receives just _one_ item -- the next item. Once the channel is closed, the receivers always receive _closed_ instead -- so they must handle the possibility of nothing.

Closing is interesting. It prevents additional input, but workers are still free to try to send -- it just won't succeed. But senders are in their own process -- it is no big deal if the send doesn't succeed -- it should be up to them if they want to stop, given that senders and receivers can _both_ try to close the channel. So the sender itself should _check_ if the channel is still open, if it wants to send without infinitely looping. Or sending should return a _boolean_, if it's possible it might have failed. It should not be an _error_, though -- it is not a failure of the sending process to send to a closed channel. There is no issue with work continuing on the current thread in this way.

We may want to wrap Channels with a newtype for Readonly and Writeonly, so that we can protect the Channel from operations where we want the Owner to be the only one who reads or writes. But at the same time, we don't want to risk _deadlocks_ -- where we stop receiving, but the caller is still sending. So things like 'forEach' would be useful.

But close is thus interesting. It is ... something of a _signal_. It signals that no further data will be handled. The Receiver naturally experiences it in the data. But it's something _either_ partner could signal and respond to, and should expect to respond to. It isn't owned by one or the other. Rather, it's a finishing condition that either side can check -- but typically the receiver pauses to receive, and can only receive it by receiving data about it.

To make the data type more obvious, we can use Closed/Value. Because we do NOT expect or want to compose this data type; it really does mean something different.

That is, however, no the only thing that might happen. We might also err. And this is interesting ... because an err in a Sending context does not translate into an error in the channel. It _could_, but it doesn't automatically do so. Which leaves us in a weird position -- the sender has erred, and now can never send, but the receiver is still receiving. This leaves us with an unrecoverable error.
  We could try to recover by telling the channel to 'err', but this seems meaningless -- it is not an error in the receiving context for an error to occur in the calling context. It might still be useful information to add in a Close -- that it closed because of error -- but it is not defacto true that a child would expect to care about an error from a sender, unless it was _explicitly_ part of the data model.
  This is pretty odd. The Channel is important, and something that we need, as a primitive. But handling errors is unclear. There is no clear model for what an error means, even though errors will be possible, or even frequent, especially if we're working with unknown APIs -- and this will cause bizarre hangs that we might not be able to detect. Piping is thus an unclear process.
  Thus, a channel is _not_ a clear model in the way we think it is. Why is that? It seems to be because a channel is an attempt to _link_ processes together, forming a _larger_ program -- but the channel itself cannot address that larger program that we wish to write. And the nature of this larger program is that there are multiple versions of it that would be desirable to write. We might want to write a single pipeline of separate processes that write to each other sequentially. We might want a single process to receive input from multiple other processes. We might want branched processes in some form. And we might want an error in one process to have meaning for the _entire_ composite program.
  A channel cannot handle all that. A channel can only be a minor vehicle that enables sending and receiving as a simple primitive, with no view towards the larger context. It is thus _inherently_ unsafe, even though it is a natural thing to have. But even though we have it, we _cannot_ publish a Channel on its own, because on its own, a Channel is non-cooperative and lacking any and all language to compose a large program -- that is to say, nouns and verbs are missing.
  The odd part is this: We can say that an Aff program has a result, and compose it with other programs, where the results are singular. And we can have an Aff function, that says it publishes a single result. But if want to publish _multiple_ results in lazy fashion, and say that I am doing so, things are no longer so simple. And yet I cannot simply _loop_ to obtain a value either.
  Thus, in addition to Channels, we have the idea of a Process -- a value that takes in multiple inputs of a type, and send multiple outputs. At least in theory, it should be possible to produce a Process from something as simple as an `a -> Aff b`, where each input prompts production of an output, even though it might also be expected to produce multiple outputs. For that to work, we end up needing 'read' and 'write' operations to describe when we want input, and when we send output. And in that time, we essentially are sending to a channel of some kind for input the input and output, with the ability to specify the BUFFER SIZE of each channel at the input and at the output -- although perhaps only in between processes, as in `connect p1 10 p2` to describe the fact that the two processes are connected with a buffer of 10. But ... having an active process _prevents_ us from analyzing the architecture as a whole. Specifying it descriptively is how we get the ability to design the computation and error handling in a monad. Otherwise, understanding the totality of a computation is extremely awkward -- and make no mistake, what we're attempting is a higher-level computation, higher-level than an Aff. From that perspective, even the Pipe syntax is awkward -- because composition is end-to-end and doesn't include any higher-level thinking about the entire pipeline. But it's exactly that higher-level thinking that actually enables true composition and activity, and transferability of higher models.
  
So we'd like to write:

```
do 
  a <- processA
  arrB <- groupBy 3 $ take 3 (processB a)
  catchError (do 
    validate arrB
  ) (\e -> do 
    pure []
  )
```

As a result, processA would produce one or more `a` values until it completes and produces no more. ProcessB would be started for each `a` value, and processA would suspend, not producing new values, until processB completed its processing of the a value. We would take 3 from process B -- from all its produced values, take just the first 3 as single values, and for those first 3, group them into grouops of 3 so they come together in an array. By that point, a new 'b' value would have been produced, but even still, processA is suspended, because the computation is not over -- then validation occurs, so that eventually we get a single `Process (Array b)` -- compositely, we produce arrays of B when we are launched. And when we are launched, previous processes suspend -- we guarantee that they cannot begin the production of the next value until full processing has taken place all down the chain into the output; that is, the sending portion of the Process does not complete/return until the value has made its way to the output. This means that _intermediate_ processes must finish their processing first. This guarantees that things actually look sequential; in other words, the value is not fully taken until the next step completes, and the next step itself does not complete until the next-next step completes, and so on.
  So the async production of values, at least in terms of the local chain, behaves syncronously with all phases running at roughly the same speed. In other words, we don't want _merging_. We aren't trying to do event handling, but rather data production, where the data produced is described as ALL the data that "will" be produced, eventually, in sequence -- avoiding the need to actually produce it all at once and read it all into memory. It's a lot like lazy lists, except its a computation where we can catch errors and cancel the entire thing. Existing solutions are extremely awkward by comparison to what I'm going for, or are overly type-driven. Not useful as a result.
  
Thus, what I ultimately want to write is something like 

```
fileStrings :: Process String
```

To specify a Process that produces one or more strings. The number of strings is finite, so we can attempt to fold over them, but folding _might_ be an infinite process that never completes. Or it _will_ complete in time -- but either way, it produces a computation that we _can_ start to observe the results.